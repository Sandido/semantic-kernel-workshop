{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Semantic Kernel Introduction\n",
    "\n",
    "### Overview of Semantic Kernel (SK) and Its Importance\n",
    "\n",
    "**Semantic Kernel** is an open-source SDK from Microsoft that acts as middleware between your application code and AI large language models (LLMs). It enables developers to easily integrate AI into apps by letting AI agents call code functions and by orchestrating complex tasks. SK is *lightweight* and *modular*, designed for **enterprise-grade solutions** with features like telemetry and filters for responsible AI. Major companies (including Microsoft) leverage SK because it’s flexible and **future-proof** – you can swap in new AI models as they emerge without rewriting your code. In short, SK helps build **robust, scalable AI applications** that can evolve with advancing AI capabilities.\n",
    "\n",
    "Key reasons why Semantic Kernel is important for AI application development:\n",
    "\n",
    "- **Bridging AI and Code**: SK combines natural language **prompts** with your **existing code and APIs**, allowing AI to take actions. The AI can request a function call and SK will execute that function and return results back to the model. This bridges the gap between what the AI *intends* and what your code can do.\n",
    "- **Plugins (Skills)**: You can expose functionalities (from simple math to complex business logic or external APIs) as SK **plugins**. By describing your code to the AI (via function definitions), the model can invoke these functions to fulfill user requests. This plugin architecture makes your AI solutions **modular and extensible**.\n",
    "- **Enterprise-ready**: SK includes support for **security, observability, and compliance** (e.g. integration with Azure services, monitoring, content filtering). Hooks and filters ensure you can enforce policies (for instance, prevent sensitive data leakage).\n",
    "- **Multi-modal & Future-Proof**: SK natively supports multiple AI services (OpenAI, Azure OpenAI, HuggingFace, etc.) and modalities. Chat-based APIs can be extended to voice or other modes. As new models (like vision-enabled models or better language models) come out, SK lets you plug them in without major changes.\n",
    "- **Rapid Development**: By handling the heavy lifting of prompt orchestration, function calling, and memory management, SK enables faster development of AI features. You focus on defining *what* you want the AI to do (skills, prompts) and SK handles *how* to do it. Microsoft claims that SK helps “deliver AI solutions faster than any other SDK” due to its ability to **automatically call functions**.\n",
    "\n",
    "---\n",
    "\n",
    "### Services and Core Components of SK\n",
    "\n",
    "Semantic Kernel's architecture revolves around a few core components and services:\n",
    "\n",
    "- **Kernel**: The central object that orchestrates everything. The `Kernel` holds configuration for AI services, manages plugins (skills), coordinates function calls, and maintains contextual state (memory). You typically create one Kernel instance in your app and use it to register functions and perform AI queries.\n",
    "- **AI Services**: SK connects to AI models for different tasks:\n",
    "  - *Chat Models*: e.g. Azure OpenAI GPT-4o-mini or GPT-4o for natural language generation and understanding.\n",
    "  - *Embedding Models*: for converting text to vector embeddings (used in memory/search).\n",
    "  - *Other Modalities*: connectors for images, speech, etc., if needed.\n",
    "  \n",
    "Semantic Kernel can automatically read your `.env` (automatically read from root of project) to access Azure OpenAI using the following variables:\n",
    "\n",
    "```\n",
    "AZURE_OPENAI_ENDPOINT - The endpoint it should talk to by default\n",
    "AZURE_OPENAI_API_KEY - The API Key it should use\n",
    "AZURE_OPENAI_API_VERSION - Inference API version it should use per default\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME - Model deployment name it should use per default\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME - Embedding deployment name it should use per default\n",
    "```\n",
    "\n",
    "Before you continue, make sure your `.env` (copied from `.env.example`) is filled out correctly.\n",
    "\n",
    "You configure the Kernel with the endpoints/keys for the services you need. For example, adding an Azure OpenAI chat completion service:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install semantic-kernel==1.29.0 python-dotenv==1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, please quickly restart your Jupyter kernel, so that SK can load properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from semantic_kernel.kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "# Auto-loads defaults from .env file, alternatively you can set endpoint, deployment_name and api_key directly\n",
    "chat_completion = AzureChatCompletion()\n",
    "\n",
    "kernel.add_service(chat_completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now successfully created a kernel and added a chat completion service.\n",
    "\n",
    "Similarly, you can add an embedding generation service via `kernel.add_service(text_embedding)` if performing semantic memory searches. But don't worry we will dive into this at a later stage.\n",
    "\n",
    "---\n",
    "\n",
    "### Functions and Plugins in SK\n",
    "\n",
    "**Functions** in Semantic Kernel are the actions that the AI can perform. They come in two types:\n",
    "\n",
    "- **Semantic Functions**: Backed by a prompt and LLM. For example, a function `translateToFrench` might use the prompt `\"Translate this to French: {{$input}}\"`.\n",
    "- **Native Functions**: Backed by code. For example, a Python function `sendEmail(to, subject, body)` that uses an API to send an email.\n",
    "\n",
    "These functions are typically grouped into **Plugins** (or \"Skills\"). Those are often represented as a class in Python. Grouping functions into plugins helps manage and control which capabilities are exposed to the AI.\n",
    "\n",
    "**Using Plugins/Functions**: Once registered with the kernel (via `kernel.add_function` or `kernel.add_plugin`), functions become available for invocation. They can be called directly in code via `kernel.invoke(function, input)`, or the AI model can automatically choose to invoke them as needed.\n",
    "\n",
    "SK’s plugin system is highly flexible:\n",
    "- You can load **OpenAPI** specifications or API endpoints as plugins.\n",
    "- Plugins can be shared across projects, allowing organizations to build a library of useful AI plugins.\n",
    "\n",
    "---\n",
    "\n",
    "You can add plugins to the Kernel in various ways:\n",
    "\n",
    "- **Inline Definition**: Define a prompt or function in code and register it.\n",
    "- **From Files or Classes**: Load plugins from directories or Python classes decorated appropriately.\n",
    "\n",
    "For example, to add a simple **semantic function** inline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Kernel is an open-source development kit for building AI agents and integrating advanced AI models into C#, Python, or Java applications.\n"
     ]
    }
   ],
   "source": [
    "# Define a semantic function (prompt) to generate a TL;DR summary\n",
    "prompt_template = \"{{$input}}\\n\\nTL;DR in one sentence:\"\n",
    "\n",
    "summarize_fn = kernel.add_function(\n",
    "    prompt=prompt_template,\n",
    "    function_name=\"tldr\",\n",
    "    plugin_name=\"Summarizer\",\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "# Use the function\n",
    "long_text = \"\"\"\n",
    "Semantic Kernel is a lightweight, open-source development kit that lets \n",
    "you easily build AI agents and integrate the latest AI models into your C#, \n",
    "Python, or Java codebase. It serves as an efficient middleware that enables \n",
    "rapid delivery of enterprise-grade solutions.\n",
    "\"\"\"\n",
    "\n",
    "summary = await kernel.invoke(summarize_fn, input=long_text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create a translation function\n",
    "\n",
    "Write a semantic function that translates text to a target language.\n",
    "\n",
    "<details>\n",
    "  <summary>Click to see solution</summary>\n",
    "  \n",
    "  ```python\n",
    "  # Define a semantic function (prompt) to generate a translation\n",
    "prompt_template = \"{{$input}}\\n\\nTranslate this into {{$target_lang}}:\"\n",
    "\n",
    "translate_fn = kernel.add_function(\n",
    "    prompt=prompt_template, \n",
    "    function_name=\"translator\", \n",
    "    plugin_name=\"Translator\",\n",
    "    max_tokens=50)\n",
    "\n",
    "# Use the function\n",
    "text = \"\"\"\n",
    "Semantic Kernel is a lightweight, open-source development kit that lets \n",
    "you easily build AI agents and integrate the latest AI models into your C#, \n",
    "Python, or Java codebase. It serves as an efficient middleware that enables \n",
    "rapid delivery of enterprise-grade solutions.\n",
    "\"\"\"\n",
    "\n",
    "summary = await kernel.invoke(translate_fn, input=text, target_lang=\"French\")\n",
    "print(summary)\n",
    "  ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "セマンティックカーネルは、軽量でオープンソースの開発キットだよ。これを使えば、AIエージェントを簡単に作ったり、最新のAIモデルをC#、Python、Javaのコードに簡単に組み込めるんだ。効率的なミドルウェアとして動くから、エンタープライズ向けのソリューションを素早く作れるよ。\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the previous implementation for reference, if you are stuck view the solution.\n",
    "\n",
    "# Your solution goes here\n",
    "# Define a semantic function (prompt) to translate text\n",
    "prompt = \"Translate this text to {{$target_language}} using a simple and non-formal conversational style: {{$input_text}}\"\n",
    "\n",
    "translate_fn = kernel.add_function(\n",
    "    prompt=prompt,\n",
    "    function_name=\"translate_to_japanese\",\n",
    "    plugin_name=\"Translator\",\n",
    "    max_tokens=1000,\n",
    ")\n",
    "\n",
    "# Use the function\n",
    "long_text = \"\"\"\n",
    "Semantic Kernel is a lightweight, open-source development kit that lets \n",
    "you easily build AI agents and integrate the latest AI models into your C#, \n",
    "Python, or Java codebase. It serves as an efficient middleware that enables \n",
    "rapid delivery of enterprise-grade solutions.\n",
    "\"\"\"\n",
    "\n",
    "translation = await kernel.invoke(translate_fn, target_language=\"japanese\", input_text=long_text)\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now if we want to build a Plugin with a set of **native functions** we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "\n",
    "class LightModel(TypedDict):\n",
    "    id: int\n",
    "    name: str\n",
    "    is_on: bool | None\n",
    "    brightness: int | None\n",
    "    hex: str | None\n",
    "\n",
    "\n",
    "class LightsPlugin:\n",
    "    def __init__(self, lights: list[LightModel]):\n",
    "        self.lights = lights\n",
    "\n",
    "    @kernel_function\n",
    "    async def get_lights(self) -> List[LightModel]:\n",
    "        \"\"\"Gets a list of lights and their current state.\"\"\"\n",
    "        return self.lights\n",
    "\n",
    "    @kernel_function\n",
    "    async def get_state(\n",
    "        self, id: Annotated[int, \"The ID of the light\"]\n",
    "    ) -> Optional[LightModel]:\n",
    "        \"\"\"Gets the state of a particular light.\"\"\"\n",
    "        for light in self.lights:\n",
    "            if light[\"id\"] == id:\n",
    "                return light\n",
    "        return None\n",
    "\n",
    "    @kernel_function\n",
    "    async def change_state(\n",
    "        self, id: Annotated[int, \"The ID of the light\"], new_state: LightModel\n",
    "    ) -> Optional[LightModel]:\n",
    "        \"\"\"Changes the state of the light.\"\"\"\n",
    "        for light in self.lights:\n",
    "            if light[\"id\"] == id:\n",
    "                light[\"is_on\"] = new_state.get(\"is_on\", light[\"is_on\"])\n",
    "                light[\"brightness\"] = new_state.get(\"brightness\", light[\"brightness\"])\n",
    "                light[\"hex\"] = new_state.get(\"hex\", light[\"hex\"])\n",
    "                return light\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelPlugin(name='Lights', description=None, functions={'change_state': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='change_state', plugin_name='Lights', description='Changes the state of the light.', parameters=[KernelParameterMetadata(name='id', description='The ID of the light', default_value=None, type_='int', is_required=True, type_object=<class 'int'>, schema_data={'type': 'integer', 'description': 'The ID of the light'}, include_in_function_choices=True), KernelParameterMetadata(name='new_state', description=None, default_value=None, type_='LightModel', is_required=True, type_object=<class '__main__.LightModel'>, schema_data={'type': 'object', 'properties': {'id': {'type': 'integer'}, 'name': {'type': 'string'}, 'is_on': {'type': ['boolean', 'null']}, 'brightness': {'type': ['integer', 'null']}, 'hex': {'type': ['string', 'null']}}, 'required': ['id', 'name']}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='LightModel', is_required=False, type_object=<class '__main__.LightModel'>, schema_data={'type': 'object', 'properties': {'id': {'type': 'integer'}, 'name': {'type': 'string'}, 'is_on': {'type': ['boolean', 'null']}, 'brightness': {'type': ['integer', 'null']}, 'hex': {'type': ['string', 'null']}}, 'required': ['id', 'name']}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x000001E46302B680>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x000001E4634DB080>, method=<bound method LightsPlugin.change_state of <__main__.LightsPlugin object at 0x000001E4634D8650>>, stream_method=None), 'get_lights': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='get_lights', plugin_name='Lights', description='Gets a list of lights and their current state.', parameters=[], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='list[LightModel]', is_required=True, type_object=<class 'list'>, schema_data={'type': 'array'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x000001E4634D8230>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x000001E4634D82F0>, method=<bound method LightsPlugin.get_lights of <__main__.LightsPlugin object at 0x000001E4634D8650>>, stream_method=None), 'get_state': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='get_state', plugin_name='Lights', description='Gets the state of a particular light.', parameters=[KernelParameterMetadata(name='id', description='The ID of the light', default_value=None, type_='int', is_required=True, type_object=<class 'int'>, schema_data={'type': 'integer', 'description': 'The ID of the light'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='LightModel', is_required=False, type_object=<class '__main__.LightModel'>, schema_data={'type': 'object', 'properties': {'id': {'type': 'integer'}, 'name': {'type': 'string'}, 'is_on': {'type': ['boolean', 'null']}, 'brightness': {'type': ['integer', 'null']}, 'hex': {'type': ['string', 'null']}}, 'required': ['id', 'name']}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x000001E4634D8A10>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x000001E4634D8620>, method=<bound method LightsPlugin.get_state of <__main__.LightsPlugin object at 0x000001E4634D8650>>, stream_method=None)})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dependencies for the plugin\n",
    "lights = [\n",
    "    {\"id\": 1, \"name\": \"Table Lamp\", \"is_on\": False, \"brightness\": 100, \"hex\": \"FF0000\"},\n",
    "    {\"id\": 2, \"name\": \"Porch light\", \"is_on\": False, \"brightness\": 50, \"hex\": \"00FF00\"},\n",
    "    {\"id\": 3, \"name\": \"Chandelier\", \"is_on\": True, \"brightness\": 75, \"hex\": \"0000FF\"},\n",
    "]\n",
    "\n",
    "plugin = LightsPlugin(lights=lights)\n",
    "\n",
    "kernel.add_plugin(\n",
    "    plugin=plugin,\n",
    "    plugin_name=\"Lights\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > The Table Lamp has been turned on.\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.connectors.ai.function_choice_behavior import (\n",
    "    FunctionChoiceBehavior,\n",
    ")\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (\n",
    "    AzureChatPromptExecutionSettings,\n",
    ")\n",
    "\n",
    "# Enable planning\n",
    "execution_settings = AzureChatPromptExecutionSettings()\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "# Create a history of the conversation\n",
    "history = ChatHistory()\n",
    "history.add_user_message(\"Please turn on the lamp\")\n",
    "\n",
    "# Get the response from the AI\n",
    "result = await chat_completion.get_chat_message_content(\n",
    "    chat_history=history,\n",
    "    settings=execution_settings,\n",
    "    kernel=kernel,\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Assistant > \" + str(result))\n",
    "\n",
    "# Add the message from the agent to the chat history\n",
    "history.add_message(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Function Calling\n",
    "\n",
    "One of the most powerful features of Semantic Kernel is its ability to **automatically orchestrate multi-step operations** by calling multiple functions in sequence. In this example, a **LightsPlugin** is defined with three asynchronous native functions:\n",
    "\n",
    "- **get_lights()**: Retrieves the list of lights along with their current states.\n",
    "- **get_state(id)**: Returns the state of a specific light, given its ID.\n",
    "- **change_state(id, new_state)**: Changes the state of a specified light to a new state (for example, turning it on, adjusting brightness, or changing its color).\n",
    "\n",
    "**How It Works**:\n",
    "- The kernel exposes all registered functions to the AI model.\n",
    "- When a user issues a command like *\"Turn on all the lights and give me their final state,\"* the AI model analyzes the request and plans the necessary steps:\n",
    "  1. Call **get_lights()** to retrieve all available lights.\n",
    "  2. For each light, invoke **change_state()** with a new state (e.g., setting `\"is_on\": True`).\n",
    "  3. Optionally, call **get_state()** for each light to confirm the updated status.\n",
    "- The kernel then returns a comprehensive result that reflects the final state of each light.\n",
    "\n",
    "**Example Scenario**:\n",
    "- **User Query**: *\"Turn on all the lights and tell me their status.\"*\n",
    "- **Step 1**: The system calls `LightsPlugin.get_lights()` to fetch the current list of lights.\n",
    "- **Step 2**: It iterates over the list and calls `LightsPlugin.change_state(id, new_state)` to turn each light on.\n",
    "- **Step 3**: Finally, it may call `LightsPlugin.get_state(id)` for each light to confirm the changes.\n",
    "- The final output, including the updated state for each light, is returned to the user.\n",
    "\n",
    "This automatic orchestration simplifies the management of multi-step tasks, enabling the AI to autonomously plan and execute function calls without manual intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > ### Final states of all lamps at various steps:\n",
      "\n",
      "1. **After turning off all lamps:**\n",
      "   - **Table Lamp:** Off, Brightness: 100, Color HEX: FF0000\n",
      "   - **Porch Light:** Off, Brightness: 50, Color HEX: 00FF00\n",
      "   - **Chandelier:** Off, Brightness: 75, Color HEX: 0000FF\n",
      "\n",
      "2. **After turning on all lamps:**\n",
      "   - **Table Lamp:** On, Brightness: 100, Color HEX: FF0000\n",
      "   - **Porch Light:** On, Brightness: 50, Color HEX: 00FF00\n",
      "   - **Chandelier:** On, Brightness: 75, Color HEX: 0000FF\n",
      "\n",
      "3. **After turning off the Table Lamp while others are still on:**\n",
      "   - **Table Lamp:** Off, Brightness: 100, Color HEX: FF0000\n",
      "   - **Porch Light:** On, Brightness: 50, Color HEX: 00FF00\n",
      "   - **Chandelier:** On, Brightness: 75, Color HEX: 0000FF\n"
     ]
    }
   ],
   "source": [
    "## Try other messages and observe the results\n",
    "\n",
    "# Add the message from the user to the chat history\n",
    "# history.add_user_message(\"Please turn on all the lamps\")\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import (\n",
    "    FunctionChoiceBehavior,\n",
    ")\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (\n",
    "    AzureChatPromptExecutionSettings,\n",
    ")\n",
    "\n",
    "# Enable planning\n",
    "execution_settings = AzureChatPromptExecutionSettings()\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "# Create a history of the conversation\n",
    "history = ChatHistory()\n",
    "history.add_user_message(\"Turn off all the lamps, tell me their final state, then turn them all on, and tell me their final state, \\\n",
    "    then turn off the table lamp and tell me the final state of all their lamps. \")\n",
    "\n",
    "# Get the response from the AI\n",
    "result = await chat_completion.get_chat_message_content(\n",
    "    chat_history=history,\n",
    "    settings=execution_settings,\n",
    "    kernel=kernel,\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Assistant > \" + str(result))\n",
    "\n",
    "# Add the message from the agent to the chat history\n",
    "history.add_message(result)\n",
    "\n",
    "# history.add_user_message(\"Please turn off all the lamps and give me their final state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Exercise: Creating a Weather Information Plugin\n",
    "\n",
    "In this exercise, you'll create a native plugin for Semantic Kernel that simulates a weather service. This will help you understand how to create and use native functions in a plugin.\n",
    "\n",
    "### Task:\n",
    "1. Create a `WeatherPlugin` class with the following functions:\n",
    "   - `get_current_weather(location)`: Returns the current weather for a given location\n",
    "   - `get_forecast(location, days)`: Returns a weather forecast for a specified number of days\n",
    "   - `get_weather_alert(location)`: Returns any active weather alerts for a location\n",
    "\n",
    "2. Register the plugin with the kernel and test it with a few user queries\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>Click to see solution</summary>\n",
    "  \n",
    "  ```python\n",
    "    from typing import Annotated, List, Dict\n",
    "    from semantic_kernel.functions import kernel_function\n",
    "    import random\n",
    "\n",
    "    class WeatherPlugin:\n",
    "        def __init__(self):\n",
    "            # Simulated weather data\n",
    "            self.weather_conditions = [\"Sunny\", \"Cloudy\", \"Rainy\", \"Snowy\", \"Windy\", \"Foggy\", \"Stormy\"]\n",
    "            self.temperature_ranges = {\n",
    "                \"New York\": (50, 85),\n",
    "                \"London\": (45, 75),\n",
    "                \"Tokyo\": (55, 90),\n",
    "                \"Sydney\": (60, 95),\n",
    "                \"Paris\": (48, 80),\n",
    "                \"Default\": (40, 100)\n",
    "            }\n",
    "            \n",
    "            # Simulated alerts\n",
    "            self.alerts = {\n",
    "                \"New York\": \"Heat advisory in effect\",\n",
    "                \"Tokyo\": \"Typhoon warning for coastal areas\",\n",
    "                \"Sydney\": None,\n",
    "                \"London\": None,\n",
    "                \"Paris\": \"Air quality warning\"\n",
    "            }\n",
    "        \n",
    "        @kernel_function\n",
    "        async def get_current_weather(\n",
    "            self,\n",
    "            location: Annotated[str, \"The city name to get weather for\"]\n",
    "        ) -> Dict:\n",
    "            \"\"\"Gets the current weather for a specified location.\"\"\"\n",
    "            temp_range = self.temperature_ranges.get(location, self.temperature_ranges[\"Default\"])\n",
    "            temperature = random.randint(temp_range[0], temp_range[1])\n",
    "            condition = random.choice(self.weather_conditions)\n",
    "            \n",
    "            return {\n",
    "                \"location\": location,\n",
    "                \"temperature\": temperature,\n",
    "                \"condition\": condition,\n",
    "                \"humidity\": random.randint(30, 95),\n",
    "                \"wind_speed\": random.randint(0, 30)\n",
    "            }\n",
    "        \n",
    "        @kernel_function\n",
    "        async def get_forecast(\n",
    "            self,\n",
    "            location: Annotated[str, \"The city name to get forecast for\"],\n",
    "            days: Annotated[int, \"Number of days for the forecast\"] = 3\n",
    "        ) -> List[Dict]:\n",
    "            \"\"\"Gets a weather forecast for a specified number of days.\"\"\"\n",
    "            forecast = []\n",
    "            temp_range = self.temperature_ranges.get(location, self.temperature_ranges[\"Default\"])\n",
    "            \n",
    "            for i in range(days):\n",
    "                forecast.append({\n",
    "                    \"day\": i + 1,\n",
    "                    \"temperature\": random.randint(temp_range[0], temp_range[1]),\n",
    "                    \"condition\": random.choice(self.weather_conditions),\n",
    "                    \"humidity\": random.randint(30, 95),\n",
    "                    \"wind_speed\": random.randint(0, 30)\n",
    "                })\n",
    "            \n",
    "            return forecast\n",
    "        \n",
    "        @kernel_function\n",
    "        async def get_weather_alert(\n",
    "            self,\n",
    "            location: Annotated[str, \"The city name to check for weather alerts\"]\n",
    "        ) -> Dict:\n",
    "            \"\"\"Gets any active weather alerts for a location.\"\"\"\n",
    "            alert = self.alerts.get(location)\n",
    "            \n",
    "            return {\n",
    "                \"location\": location,\n",
    "                \"has_alert\": alert is not None,\n",
    "                \"alert_message\": alert if alert else \"No active alerts\"\n",
    "            }\n",
    "\n",
    "    # Usage example:\n",
    "    # Create the plugin\n",
    "    weather_plugin = WeatherPlugin()\n",
    "\n",
    "    # Register with kernel\n",
    "    kernel.add_plugin(\n",
    "        plugin=weather_plugin,\n",
    "        plugin_name=\"Weather\"\n",
    "    )\n",
    "\n",
    "    # Test with a user query\n",
    "    history = ChatHistory()\n",
    "    history.add_user_message(\"What's the weather like in Tokyo and are there any alerts?\")\n",
    "\n",
    "    # Get response using function calling\n",
    "    execution_settings = AzureChatPromptExecutionSettings()\n",
    "    execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "    result = await chat_completion.get_chat_message_content(\n",
    "        chat_history=history,\n",
    "        settings=execution_settings,\n",
    "        kernel=kernel,\n",
    "    )\n",
    "\n",
    "    print(\"Assistant > \" + str(result))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "### Learning Objectives:\n",
    "- Creating a native plugin with multiple functions\n",
    "- Using type annotations for function parameters\n",
    "- Registering a plugin with the kernel\n",
    "- Testing the plugin with natural language queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > The weather in Tokyo is currently 78°F with cloudy skies. The humidity is at 54%, and there's a wind speed of 28 mph.\n",
      "\n",
      "Additionally, there is an active weather alert: **Typhoon warning for coastal areas**.\n"
     ]
    }
   ],
   "source": [
    "## TODO: Exercise\n",
    "\n",
    "from typing import Annotated, List, Dict\n",
    "from semantic_kernel.functions import kernel_function\n",
    "import random\n",
    "\n",
    "class WeatherPlugin:\n",
    "    def __init__(self):\n",
    "        # Simulated weather data\n",
    "        self.weather_conditions = [\"Sunny\", \"Cloudy\", \"Rainy\", \"Snowy\", \"Windy\", \"Foggy\", \"Stormy\"]\n",
    "        self.temperature_ranges = {\n",
    "            \"New York\": (50, 85),\n",
    "            \"London\": (45, 75),\n",
    "            \"Tokyo\": (55, 90),\n",
    "            \"Sydney\": (60, 95),\n",
    "            \"Paris\": (48, 80),\n",
    "            \"Default\": (40, 100)\n",
    "        }\n",
    "\n",
    "        # Simulated alerts\n",
    "        self.alerts = {\n",
    "            \"New York\": \"Heat advisory in effect\",\n",
    "            \"Tokyo\": \"Typhoon warning for coastal areas\",\n",
    "            \"Sydney\": None,\n",
    "            \"London\": None,\n",
    "            \"Paris\": \"Air quality warning\"\n",
    "        }\n",
    "\n",
    "    @kernel_function\n",
    "    async def get_current_weather(\n",
    "        self,\n",
    "        location: Annotated[str, \"The city name to get weather for\"]\n",
    "    ) -> Dict:\n",
    "        \"\"\"Gets the current weather for a specified location.\"\"\"\n",
    "        temp_range = self.temperature_ranges.get(location, self.temperature_ranges[\"Default\"])\n",
    "        temperature = random.randint(temp_range[0], temp_range[1])\n",
    "        condition = random.choice(self.weather_conditions)\n",
    "\n",
    "        return {\n",
    "            \"location\": location,\n",
    "            \"temperature\": temperature,\n",
    "            \"condition\": condition,\n",
    "            \"humidity\": random.randint(30, 95),\n",
    "            \"wind_speed\": random.randint(0, 30)\n",
    "        }\n",
    "\n",
    "    @kernel_function\n",
    "    async def get_forecast(\n",
    "        self,\n",
    "        location: Annotated[str, \"The city name to get forecast for\"],\n",
    "        days: Annotated[int, \"Number of days for the forecast\"] = 3\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Gets a weather forecast for a specified number of days.\"\"\"\n",
    "        forecast = []\n",
    "        temp_range = self.temperature_ranges.get(location, self.temperature_ranges[\"Default\"])\n",
    "\n",
    "        for i in range(days):\n",
    "            forecast.append({\n",
    "                \"day\": i + 1,\n",
    "                \"temperature\": random.randint(temp_range[0], temp_range[1]),\n",
    "                \"condition\": random.choice(self.weather_conditions),\n",
    "                \"humidity\": random.randint(30, 95),\n",
    "                \"wind_speed\": random.randint(0, 30)\n",
    "            })\n",
    "\n",
    "        return forecast\n",
    "\n",
    "    @kernel_function\n",
    "    async def get_weather_alert(\n",
    "        self,\n",
    "        location: Annotated[str, \"The city name to check for weather alerts\"]\n",
    "    ) -> Dict:\n",
    "        \"\"\"Gets any active weather alerts for a location.\"\"\"\n",
    "        alert = self.alerts.get(location)\n",
    "\n",
    "        return {\n",
    "            \"location\": location,\n",
    "            \"has_alert\": alert is not None,\n",
    "            \"alert_message\": alert if alert else \"No active alerts\"\n",
    "        }\n",
    "\n",
    "# Usage example:\n",
    "# Create the plugin\n",
    "weather_plugin = WeatherPlugin()\n",
    "\n",
    "# Register with kernel\n",
    "kernel.add_plugin(\n",
    "    plugin=weather_plugin,\n",
    "    plugin_name=\"Weather\"\n",
    ")\n",
    "\n",
    "# Test with a user query\n",
    "history = ChatHistory()\n",
    "history.add_user_message(\"What's the weather like in Tokyo and are there any alerts?\")\n",
    "\n",
    "# Get response using function calling\n",
    "execution_settings = AzureChatPromptExecutionSettings()\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "result = await chat_completion.get_chat_message_content(\n",
    "    chat_history=history,\n",
    "    settings=execution_settings,\n",
    "    kernel=kernel,\n",
    ")\n",
    "\n",
    "print(\"Assistant > \" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filters in Semantic Kernel\n",
    "\n",
    "Filters provide a layer of control and visibility over function execution, ensuring responsible AI practices and enterprise-grade security. They allow you to:\n",
    "\n",
    "- **Validate Permissions:**  \n",
    "  For example, a filter can check user permissions before initiating an approval flow.\n",
    "\n",
    "- **Intercept Function Execution:**  \n",
    "  - **Function Invocation Filter:**  \n",
    "    Runs every time a function is called; it can access function details, handle exceptions, override results (e.g., for caching or responsible AI), or retry on failure.\n",
    "  - **Prompt Render Filter:**  \n",
    "    Triggered before a prompt is rendered; it allows you to view or modify the prompt and even override the result to prevent submission.\n",
    "  - **Auto Function Invocation Filter:**  \n",
    "    Works within automatic function calling, providing additional context (like chat history and iteration counters) and can terminate the process early if needed.\n",
    "\n",
    "Each filter receives a context object with execution details and must call the next delegate (or callback) to continue the execution chain. Filters can be registered either by using the `add_filter` method on the Kernel or via the `@kernel.filter` decorator.\n",
    "\n",
    "\n",
    "One of the things that I would like to improve in our plugin implementation is to add debugging, that way I can integrate with external systems for auditing purposes\n",
    "\n",
    "Lets implement that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure the logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Awaitable, Callable\n",
    "from semantic_kernel.filters import FunctionInvocationContext\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "async def logger_filter(\n",
    "    context: FunctionInvocationContext,\n",
    "    next: Callable[[FunctionInvocationContext], Awaitable[None]],\n",
    ") -> None:\n",
    "    logger.info(f\"FunctionInvoking - {context.function.plugin_name}.{context.function.name}\")\n",
    "\n",
    "    await next(context)\n",
    "\n",
    "    logger.info(f\"FunctionInvoked - {context.function.plugin_name}.{context.function.name}\")\n",
    "\n",
    "\n",
    "kernel.add_filter(\"function_invocation\", logger_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 19:06:55,171 - INFO - HTTP Request: POST https://azopenaires.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-03-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-05-17 19:06:55,173 - INFO - OpenAI usage: CompletionUsage(completion_tokens=14, prompt_tokens=419, total_tokens=433, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "2025-05-17 19:06:55,175 - INFO - processing 1 tool calls in parallel.\n",
      "2025-05-17 19:06:55,176 - INFO - Calling Lights-get_lights function with args: {}\n",
      "2025-05-17 19:06:55,177 - INFO - Function Lights-get_lights invoking.\n",
      "2025-05-17 19:06:55,177 - INFO - FunctionInvoking - Lights.get_lights\n",
      "2025-05-17 19:06:55,178 - INFO - FunctionInvoked - Lights.get_lights\n",
      "2025-05-17 19:06:55,178 - INFO - Function Lights-get_lights succeeded.\n",
      "2025-05-17 19:06:55,178 - INFO - Function completed. Duration: 0.001599s\n",
      "2025-05-17 19:06:56,726 - INFO - HTTP Request: POST https://azopenaires.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-03-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-05-17 19:06:56,730 - INFO - OpenAI usage: CompletionUsage(completion_tokens=148, prompt_tokens=544, total_tokens=692, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "2025-05-17 19:06:56,732 - INFO - processing 3 tool calls in parallel.\n",
      "2025-05-17 19:06:56,733 - INFO - Calling Lights-change_state function with args: {\"id\": 1, \"new_state\": {\"id\": 1, \"name\": \"Table Lamp\", \"is_on\": true, \"brightness\": 100, \"hex\": \"FF0000\"}}\n",
      "2025-05-17 19:06:56,734 - INFO - Function Lights-change_state invoking.\n",
      "2025-05-17 19:06:56,736 - INFO - FunctionInvoking - Lights.change_state\n",
      "2025-05-17 19:06:56,737 - INFO - FunctionInvoked - Lights.change_state\n",
      "2025-05-17 19:06:56,738 - INFO - Function Lights-change_state succeeded.\n",
      "2025-05-17 19:06:56,740 - INFO - Function completed. Duration: 0.003407s\n",
      "2025-05-17 19:06:56,742 - INFO - Calling Lights-change_state function with args: {\"id\": 2, \"new_state\": {\"id\": 2, \"name\": \"Porch light\", \"is_on\": true, \"brightness\": 50, \"hex\": \"00FF00\"}}\n",
      "2025-05-17 19:06:56,743 - INFO - Function Lights-change_state invoking.\n",
      "2025-05-17 19:06:56,744 - INFO - FunctionInvoking - Lights.change_state\n",
      "2025-05-17 19:06:56,745 - INFO - FunctionInvoked - Lights.change_state\n",
      "2025-05-17 19:06:56,746 - INFO - Function Lights-change_state succeeded.\n",
      "2025-05-17 19:06:56,748 - INFO - Function completed. Duration: 0.003652s\n",
      "2025-05-17 19:06:56,749 - INFO - Calling Lights-change_state function with args: {\"id\": 3, \"new_state\": {\"id\": 3, \"name\": \"Chandelier\", \"is_on\": true, \"brightness\": 75, \"hex\": \"0000FF\"}}\n",
      "2025-05-17 19:06:56,750 - INFO - Function Lights-change_state invoking.\n",
      "2025-05-17 19:06:56,750 - INFO - FunctionInvoking - Lights.change_state\n",
      "2025-05-17 19:06:56,751 - INFO - FunctionInvoked - Lights.change_state\n",
      "2025-05-17 19:06:56,752 - INFO - Function Lights-change_state succeeded.\n",
      "2025-05-17 19:06:56,753 - INFO - Function completed. Duration: 0.002396s\n",
      "2025-05-17 19:06:57,862 - INFO - HTTP Request: POST https://azopenaires.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-03-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-05-17 19:06:57,865 - INFO - OpenAI usage: CompletionUsage(completion_tokens=88, prompt_tokens=807, total_tokens=895, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > All lamps have been turned on. Here is their final state:\n",
      "\n",
      "1. **Table Lamp**: On, Brightness: 100, Color: #FF0000 (Red)\n",
      "2. **Porch light**: On, Brightness: 50, Color: #00FF00 (Green)\n",
      "3. **Chandelier**: On, Brightness: 75, Color: #0000FF (Blue)\n"
     ]
    }
   ],
   "source": [
    "history.add_user_message(\"Please turn on all the lamps and give me their final state\")\n",
    "\n",
    "# Get the response from the AI\n",
    "result = await chat_completion.get_chat_message_content(\n",
    "    chat_history=history,\n",
    "    settings=execution_settings,\n",
    "    kernel=kernel,\n",
    ")\n",
    "\n",
    "# Add the message from the agent to the chat history\n",
    "history.add_message(result)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(\"Assistant > \" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should look something like this\n",
    "\n",
    "```shell\n",
    "2025-02-27 15:21:00,969 - INFO - HTTP Request: POST <AOAI Endpoint> \"HTTP/1.1 200 OK\"\n",
    "2025-02-27 15:21:00,971 - INFO - OpenAI usage: CompletionUsage(completion_tokens=84, prompt_tokens=407, total_tokens=491, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
    "2025-02-27 15:21:00,971 - INFO - processing 2 tool calls in parallel.\n",
    "2025-02-27 15:21:00,972 - INFO - Calling Lights-change_state function with args: {\"id\": 2, \"new_state\": {\"id\": 2, \"name\": \"Porch light\", \"is_on\": true}}\n",
    "2025-02-27 15:21:00,972 - INFO - Function Lights-change_state invoking.\n",
    "2025-02-27 15:21:00,972 - INFO - FunctionInvoking - Lights.change_state\n",
    "2025-02-27 15:21:00,972 - INFO - FunctionInvoked - Lights.change_state\n",
    "2025-02-27 15:21:00,973 - INFO - Function Lights-change_state succeeded.\n",
    "2025-02-27 15:21:00,973 - INFO - Function completed. Duration: 0.000904s\n",
    "2025-02-27 15:21:00,974 - INFO - Calling Lights-change_state function with args: {\"id\": 3, \"new_state\": {\"id\": 3, \"name\": \"Chandelier\", \"is_on\": true}}\n",
    "2025-02-27 15:21:00,974 - INFO - Function Lights-change_state invoking.\n",
    "2025-02-27 15:21:00,974 - INFO - FunctionInvoking - Lights.change_state\n",
    "2025-02-27 15:21:00,974 - INFO - FunctionInvoked - Lights.change_state\n",
    "2025-02-27 15:21:00,974 - INFO - Function Lights-change_state succeeded.\n",
    "2025-02-27 15:21:00,975 - INFO - Function completed. Duration: 0.000567s\n",
    "2025-02-27 15:21:06,333 - INFO - HTTP Request: POST <AOAI Endpoint> \"HTTP/1.1 200 OK\"\n",
    "2025-02-27 15:21:06,335 - INFO - OpenAI usage: CompletionUsage(completion_tokens=120, prompt_tokens=572, total_tokens=692, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
    "Assistant > All the lights are now turned on. Here's the final state of each lamp:\n",
    "\n",
    "1. **Table Lamp**\n",
    "   - Status: On\n",
    "   - Brightness: 100%\n",
    "   - Color: Red (#FF0000)\n",
    "\n",
    "2. **Porch Light**\n",
    "   - Status: On\n",
    "   - Brightness: 50%\n",
    "   - Color: Green (#00FF00)\n",
    "\n",
    "3. **Chandelier**\n",
    "   - Status: On\n",
    "   - Brightness: 75%\n",
    "   - Color: Blue (#0000FF)\n",
    "\n",
    "If you need any further adjustments, just let me know!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Creating a Content Filter\n",
    "\n",
    "In this exercise, you'll create a filter that monitors and potentially modifies content flowing through the Semantic Kernel pipeline. This will help you understand how to implement safety and compliance measures in AI applications.\n",
    "\n",
    "### Task:\n",
    "1. Create a content filter that detects and redacts sensitive information (like credit card numbers, emails, etc.)\n",
    "2. Implement the filter as a pre-processing step for user inputs and a post-processing step for AI outputs\n",
    "3. Test the filter with various inputs containing sensitive information\n",
    "\n",
    "<details>\n",
    "  <summary>Click to see solution</summary>\n",
    "  \n",
    "  ```python\n",
    "import re\n",
    "from typing import Any, Coroutine\n",
    "from collections.abc import Callable as ABCCallable\n",
    "import logging\n",
    "from semantic_kernel.filters import FunctionInvocationContext\n",
    "from semantic_kernel.functions import FunctionResult\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Regular expressions for sensitive data patterns\n",
    "PATTERNS = {\n",
    "    'credit_card': r'\\b(?:\\d{4}[-\\s]?){3}\\d{4}\\b',  # Credit card format: XXXX-XXXX-XXXX-XXXX\n",
    "    'email': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',  # Email addresses\n",
    "    'phone': r'\\b(?:\\+\\d{1,3}[-\\s]?)?\\(?\\d{3}\\)?[-\\s]?\\d{3}[-\\s]?\\d{4}\\b',  # Phone numbers\n",
    "    'ssn': r'\\b\\d{3}[-\\s]?\\d{2}[-\\s]?\\d{4}\\b',  # Social Security Numbers (US)\n",
    "}\n",
    "\n",
    "class ContentFilter:\n",
    "    def __init__(self, patterns=PATTERNS):\n",
    "        self.patterns = patterns\n",
    "        \n",
    "    def redact_sensitive_info(self, text):\n",
    "        \"\"\"Redact sensitive information from text.\"\"\"\n",
    "        result = text\n",
    "        detected = []\n",
    "        \n",
    "        for pattern_name, pattern in self.patterns.items():\n",
    "            matches = re.finditer(pattern, result)\n",
    "            for match in matches:\n",
    "                detected.append(f\"{pattern_name}: {match.group()}\")\n",
    "                result = result.replace(match.group(), f\"[REDACTED {pattern_name.upper()}]\")\n",
    "        \n",
    "        return result, detected\n",
    "\n",
    "# Create a pre-processing filter for user inputs\n",
    "async def input_filter(\n",
    "    context: FunctionInvocationContext,\n",
    "    next: ABCCallable[[FunctionInvocationContext], Coroutine[Any, Any, None]]\n",
    ") -> None:\n",
    "    content_filter = ContentFilter()\n",
    "    \n",
    "    # Check if there's an input parameter\n",
    "    if 'input' in context.arguments:\n",
    "        original_input = context.arguments['input']\n",
    "        \n",
    "        # Apply the filter\n",
    "        filtered_input, detected = content_filter.redact_sensitive_info(original_input)\n",
    "        \n",
    "        if detected:\n",
    "            logger.warning(f\"Sensitive information detected in input: {', '.join(detected)}\")\n",
    "            \n",
    "        # Replace the original input with the filtered version\n",
    "        context.arguments['input'] = filtered_input\n",
    "    \n",
    "    # Continue to the next filter or function\n",
    "    await next(context)\n",
    "\n",
    "# Create a post-processing filter for AI outputs\n",
    "async def output_filter(\n",
    "    context: FunctionInvocationContext,\n",
    "    next: ABCCallable[[FunctionInvocationContext], Coroutine[Any, Any, None]]\n",
    ") -> None:\n",
    "    # Continue to the next filter or function first\n",
    "    await next(context)\n",
    "    \n",
    "    content_filter = ContentFilter()\n",
    "    \n",
    "    # Check if there's a result to filter\n",
    "    if context.result:\n",
    "        original_output = str(context.result)\n",
    "        \n",
    "        # Apply the filter\n",
    "        filtered_output, detected = content_filter.redact_sensitive_info(original_output)\n",
    "        \n",
    "        if detected:\n",
    "            logger.warning(f\"Sensitive information detected in output: {', '.join(detected)}\")\n",
    "         \n",
    "        # Create a new FunctionResult with the filtered output\n",
    "        context.result = FunctionResult(\n",
    "            function=context.function.metadata,\n",
    "            value=filtered_output,\n",
    "            metadata=context.result.metadata if hasattr(context.result, 'metadata') else {}\n",
    "        )\n",
    "\n",
    "async def test_content_filters():\n",
    "    \n",
    "    # Register the filters with the kernel\n",
    "    kernel.add_filter(\"function_invocation\", input_filter)\n",
    "    kernel.add_filter(\"function_invocation\", output_filter)\n",
    "    \n",
    "    # Create a simple semantic function\n",
    "    echo_prompt = \"{{$input}}\"\n",
    "    echo_fn = kernel.add_function(\n",
    "        prompt=echo_prompt,\n",
    "        function_name=\"echo\",\n",
    "        plugin_name=\"TestPlugin\"\n",
    "    )\n",
    "    \n",
    "    # Test with sensitive information\n",
    "    test_inputs = [\n",
    "        \"My credit card number is 4111-1111-1111-1111\",\n",
    "        \"Contact me at john.doe@example.com or call 555-123-4567\",\n",
    "        \"My SSN is 123-45-6789\",\n",
    "        \"Here's my info: john.doe@example.com, 4111-1111-1111-1111, 555-123-4567\",\n",
    "        \"What are the services that you can offer?\"\n",
    "    ]\n",
    "    \n",
    "    for input_text in test_inputs:\n",
    "        print(f\"\\nOriginal Input: {input_text}\")\n",
    "        result = await kernel.invoke(echo_fn, input=input_text)\n",
    "        print(f\"Filtered Output: {result}\")\n",
    "\n",
    "await test_content_filters()\n",
    "```\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Learning Objectives:\n",
    "- Creating and registering filters in Semantic Kernel\n",
    "- Implementing pre-processing and post-processing logic\n",
    "- Using regular expressions for pattern matching\n",
    "- Understanding the filter execution pipeline\n",
    "- Logging and monitoring sensitive information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Filters in SK and Their Use Cases\n",
    "\n",
    "To summarize, in any AI application, it’s important to control input/output and function execution for security, privacy, and correctness. **Filters** in Semantic Kernel act as middleware or interceptors in the execution pipeline.\n",
    "\n",
    "**Use Cases for Filters**:\n",
    "- **Security/Policy**: Prevent sensitive data from being sent to the AI.\n",
    "- **Validation**: Check function arguments before execution.\n",
    "- **Error Handling**: Catch exceptions and provide default results.\n",
    "- **Logging/Monitoring**: Log each function call and its response.\n",
    "- **Post-processing**: Modify outputs before they’re returned to the AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 19:31:34,096 - INFO - Function TestPlugin-echo invoking.\n",
      "2025-05-17 19:31:34,097 - INFO - FunctionInvoking - TestPlugin.echo\n",
      "2025-05-17 19:31:34,097 - WARNING - Sensitive information detected in input: credit_card: 4111-1111-1111-1111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Input: My credit card number is 4111-1111-1111-1111\n",
      "filtered input!!!!!!: My credit card number is [REDACTED CREDIT_CARD]\n",
      "filtered input!!!!!!: My credit card number is [REDACTED CREDIT_CARD]\n",
      "filtered input!!!!!!: My credit card number is [REDACTED CREDIT_CARD]\n",
      "filtered input!!!!!!: My credit card number is [REDACTED CREDIT_CARD]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 19:31:35,376 - INFO - HTTP Request: POST https://azopenaires.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-03-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-05-17 19:31:35,377 - INFO - OpenAI usage: CompletionUsage(completion_tokens=51, prompt_tokens=20, total_tokens=71, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "2025-05-17 19:31:35,378 - INFO - FunctionInvoked - TestPlugin.echo\n",
      "2025-05-17 19:31:35,378 - INFO - Function TestPlugin-echo succeeded.\n",
      "2025-05-17 19:31:35,378 - INFO - Function completed. Duration: 1.281677s\n",
      "2025-05-17 19:31:35,379 - INFO - Function TestPlugin-echo invoking.\n",
      "2025-05-17 19:31:35,379 - INFO - FunctionInvoking - TestPlugin.echo\n",
      "2025-05-17 19:31:35,380 - WARNING - Sensitive information detected in input: email: john.doe@example.com, phone: 555-123-4567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Output: I'm sorry, but I can't assist with that. Please do not share sensitive personal information, such as credit card numbers, online for your own safety and privacy. If you have a question or need help, feel free to ask without including private details!\n",
      "\n",
      "Original Input: Contact me at john.doe@example.com or call 555-123-4567\n",
      "filtered input!!!!!!: Contact me at [REDACTED EMAIL] or call [REDACTED PHONE]\n",
      "filtered input!!!!!!: Contact me at [REDACTED EMAIL] or call [REDACTED PHONE]\n",
      "filtered input!!!!!!: Contact me at [REDACTED EMAIL] or call [REDACTED PHONE]\n",
      "filtered input!!!!!!: Contact me at [REDACTED EMAIL] or call [REDACTED PHONE]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 19:31:35,873 - INFO - HTTP Request: POST https://azopenaires.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-03-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-05-17 19:31:35,874 - INFO - OpenAI usage: CompletionUsage(completion_tokens=25, prompt_tokens=26, total_tokens=51, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "2025-05-17 19:31:35,875 - INFO - FunctionInvoked - TestPlugin.echo\n",
      "2025-05-17 19:31:35,875 - INFO - Function TestPlugin-echo succeeded.\n",
      "2025-05-17 19:31:35,876 - INFO - Function completed. Duration: 0.496178s\n",
      "2025-05-17 19:31:35,876 - INFO - Function TestPlugin-echo invoking.\n",
      "2025-05-17 19:31:35,876 - INFO - FunctionInvoking - TestPlugin.echo\n",
      "2025-05-17 19:31:35,877 - WARNING - Sensitive information detected in input: ssn: 123-45-6789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Output: I'm sorry, I can't assist with direct communication methods such as email or phone. How can I assist you here instead?\n",
      "\n",
      "Original Input: My SSN is 123-45-6789\n",
      "filtered input!!!!!!: My SSN is [REDACTED SSN]\n",
      "filtered input!!!!!!: My SSN is [REDACTED SSN]\n",
      "filtered input!!!!!!: My SSN is [REDACTED SSN]\n",
      "filtered input!!!!!!: My SSN is [REDACTED SSN]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 19:31:36,652 - INFO - HTTP Request: POST https://azopenaires.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-03-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-05-17 19:31:36,655 - INFO - OpenAI usage: CompletionUsage(completion_tokens=46, prompt_tokens=19, total_tokens=65, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "2025-05-17 19:31:36,657 - INFO - FunctionInvoked - TestPlugin.echo\n",
      "2025-05-17 19:31:36,658 - INFO - Function TestPlugin-echo succeeded.\n",
      "2025-05-17 19:31:36,659 - INFO - Function completed. Duration: 0.781966s\n",
      "2025-05-17 19:31:36,659 - INFO - Function TestPlugin-echo invoking.\n",
      "2025-05-17 19:31:36,660 - INFO - FunctionInvoking - TestPlugin.echo\n",
      "2025-05-17 19:31:36,661 - WARNING - Sensitive information detected in input: credit_card: 4111-1111-1111-1111, email: john.doe@example.com, phone: 555-123-4567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Output: I'm sorry, but I can't process or store sensitive personal information like Social Security Numbers (SSNs). Please make sure to keep your personal information private and secure. Let me know if there's anything else I can assist you with!\n",
      "\n",
      "Original Input: Here's my info: john.doe@example.com, 4111-1111-1111-1111, 555-123-4567\n",
      "filtered input!!!!!!: Here's my info: [REDACTED EMAIL], [REDACTED CREDIT_CARD], [REDACTED PHONE]\n",
      "filtered input!!!!!!: Here's my info: [REDACTED EMAIL], [REDACTED CREDIT_CARD], [REDACTED PHONE]\n",
      "filtered input!!!!!!: Here's my info: [REDACTED EMAIL], [REDACTED CREDIT_CARD], [REDACTED PHONE]\n",
      "filtered input!!!!!!: Here's my info: [REDACTED EMAIL], [REDACTED CREDIT_CARD], [REDACTED PHONE]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 19:31:37,549 - INFO - HTTP Request: POST https://azopenaires.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-03-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-05-17 19:31:37,550 - INFO - OpenAI usage: CompletionUsage(completion_tokens=53, prompt_tokens=33, total_tokens=86, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "2025-05-17 19:31:37,551 - INFO - FunctionInvoked - TestPlugin.echo\n",
      "2025-05-17 19:31:37,551 - INFO - Function TestPlugin-echo succeeded.\n",
      "2025-05-17 19:31:37,551 - INFO - Function completed. Duration: 0.891299s\n",
      "2025-05-17 19:31:37,552 - INFO - Function TestPlugin-echo invoking.\n",
      "2025-05-17 19:31:37,552 - INFO - FunctionInvoking - TestPlugin.echo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Output: I'm sorry, but I can't process or store sensitive personal information such as email addresses, credit card details, or phone numbers. For your privacy and security, please avoid sharing such information online. If you have a question or need assistance, feel free to ask!\n",
      "\n",
      "Original Input: What are the services that you can offer?\n",
      "filtered input!!!!!!: What are the services that you can offer?\n",
      "filtered input!!!!!!: What are the services that you can offer?\n",
      "filtered input!!!!!!: What are the services that you can offer?\n",
      "filtered input!!!!!!: What are the services that you can offer?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 19:31:41,418 - INFO - HTTP Request: POST https://azopenaires.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-03-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-05-17 19:31:41,428 - INFO - OpenAI usage: CompletionUsage(completion_tokens=530, prompt_tokens=16, total_tokens=546, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "2025-05-17 19:31:41,450 - INFO - FunctionInvoked - TestPlugin.echo\n",
      "2025-05-17 19:31:41,453 - INFO - Function TestPlugin-echo succeeded.\n",
      "2025-05-17 19:31:41,455 - INFO - Function completed. Duration: 3.902318s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Output: I can provide a wide range of services to help you with information, problem-solving, learning, and creativity. Here's a breakdown of the main areas I can assist with:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. General Information & Knowledge**\n",
      "- Provide explanations on a wide variety of topics (science, technology, history, arts, culture, etc.).\n",
      "- Clarify complex concepts in simple terms.\n",
      "- Summarize research, articles, or technical data.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Writing & Editing**\n",
      "- Draft and edit emails, essays, reports, blog posts, or articles.\n",
      "- Help with creative writing (short stories, poetry, dialogues, ideas).\n",
      "- Proofread and improve grammar, structure, and tone for written content.\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Learning & Education Support**\n",
      "- Offer tutoring in subjects like math, science, literature, etc.\n",
      "- Generate study guides, summaries, or flashcards.\n",
      "- Explain concepts or provide test preparation strategies.\n",
      "- Help you learn a new skill or subject step by step.\n",
      "\n",
      "---\n",
      "\n",
      "### **4. Code & Tech Assistance**\n",
      "- Write, debug, or explain code (Python, JavaScript, HTML, etc.).\n",
      "- Help brainstorm and troubleshoot software development ideas.\n",
      "- Provide basic tech advice (e.g., app use, computer troubleshooting).\n",
      "\n",
      "---\n",
      "\n",
      "### **5. Career & Professional Development**\n",
      "- Create or improve resumes and cover letters.\n",
      "- Offer tips for job interviews and networking strategies.\n",
      "- Suggest ideas for career growth and skill development.\n",
      "\n",
      "---\n",
      "\n",
      "### **6. Creative Brainstorming & Idea Generation**\n",
      "- Generate ideas for projects, events, businesses, hobbies, etc.\n",
      "- Help with character development, world-building, or storytelling.\n",
      "- Assist with branding, slogans, or marketing ideas.\n",
      "\n",
      "---\n",
      "\n",
      "### **7. Mental Wellness & Productivity Tips**\n",
      "- Share productivity techniques (e.g., time management, focus strategies).\n",
      "- Help with problem-solving or overcoming creative blocks.\n",
      "- Provide motivational support or suggest self-care practices.\n",
      "\n",
      "---\n",
      "\n",
      "### **8. Translation & Language Assistance**\n",
      "- Translate text into multiple languages (basic level).\n",
      "- Help with grammar, vocabulary, and phrasing for language learners.\n",
      "\n",
      "---\n",
      "\n",
      "### **9. Research & Decision Support**\n",
      "- Offer pros and cons for choices or decisions (e.g., buying products, career shifts).\n",
      "- Conduct exploratory research on a chosen topic.\n",
      "- Assist in organizing or analyzing gathered information.\n",
      "\n",
      "---\n",
      "\n",
      "### **10. Personalized Recommendations**\n",
      "- Suggest books, movies, music, recipes, or hobbies tailored to your interests.\n",
      "- Provide travel tips or ideas for destinations and activities.\n",
      "\n",
      "---\n",
      "\n",
      "Feel free to ask me for anything! If it’s not listed here, ask anyway—I'll do my best to help! 😊\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import Any, Coroutine\n",
    "from collections.abc import Callable as ABCCallable\n",
    "import logging\n",
    "from semantic_kernel.filters import FunctionInvocationContext\n",
    "from semantic_kernel.functions import FunctionResult\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Regular expressions for sensitive data patterns\n",
    "PATTERNS = {\n",
    "    'credit_card': r'\\b(?:\\d{4}[-\\s]?){3}\\d{4}\\b',  # Credit card format: XXXX-XXXX-XXXX-XXXX\n",
    "    'email': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',  # Email addresses\n",
    "    'phone': r'\\b(?:\\+\\d{1,3}[-\\s]?)?\\(?\\d{3}\\)?[-\\s]?\\d{3}[-\\s]?\\d{4}\\b',  # Phone numbers\n",
    "    'ssn': r'\\b\\d{3}[-\\s]?\\d{2}[-\\s]?\\d{4}\\b',  # Social Security Numbers (US)\n",
    "}\n",
    "\n",
    "class ContentFilter:\n",
    "    def __init__(self, patterns=PATTERNS):\n",
    "        self.patterns = patterns\n",
    "\n",
    "    def redact_sensitive_info(self, text):\n",
    "        \"\"\"Redact sensitive information from text.\"\"\"\n",
    "        result = text\n",
    "        detected = []\n",
    "\n",
    "        for pattern_name, pattern in self.patterns.items():\n",
    "            matches = re.finditer(pattern, result)\n",
    "            for match in matches:\n",
    "                detected.append(f\"{pattern_name}: {match.group()}\")\n",
    "                result = result.replace(match.group(), f\"[REDACTED {pattern_name.upper()}]\")\n",
    "\n",
    "        return result, detected\n",
    "\n",
    "# Create a pre-processing filter for user inputs\n",
    "async def input_filter(\n",
    "    context: FunctionInvocationContext,\n",
    "    next: ABCCallable[[FunctionInvocationContext], Coroutine[Any, Any, None]]\n",
    ") -> None:\n",
    "    content_filter = ContentFilter()\n",
    "\n",
    "    # Check if there's an input parameter\n",
    "    if 'input' in context.arguments:\n",
    "        original_input = context.arguments['input']\n",
    "\n",
    "        # Apply the filter\n",
    "        filtered_input, detected = content_filter.redact_sensitive_info(original_input)\n",
    "        print(\"filtered input!!!!!!: \"+ filtered_input)\n",
    "        if detected:\n",
    "            logger.warning(f\"Sensitive information detected in input!!!!!: {', '.join(detected)}\")\n",
    "            logger.warning(f\"Sensitive information detected in input, filtered: {', '.join(filtered_input)}\")\n",
    "\n",
    "        # Replace the original input with the filtered version\n",
    "        context.arguments['input'] = filtered_input\n",
    "\n",
    "    # Continue to the next filter or function\n",
    "    await next(context)\n",
    "\n",
    "# Create a post-processing filter for AI outputs\n",
    "async def output_filter(\n",
    "    context: FunctionInvocationContext,\n",
    "    next: ABCCallable[[FunctionInvocationContext], Coroutine[Any, Any, None]]\n",
    ") -> None:\n",
    "    # Continue to the next filter or function first\n",
    "    await next(context)\n",
    "\n",
    "    content_filter = ContentFilter()\n",
    "\n",
    "    # Check if there's a result to filter\n",
    "    if context.result:\n",
    "        original_output = str(context.result)\n",
    "\n",
    "        # Apply the filter\n",
    "        filtered_output, detected = content_filter.redact_sensitive_info(original_output)\n",
    "\n",
    "        if detected:\n",
    "            logger.warning(f\"Sensitive information detected in output: {', '.join(detected)}\")\n",
    "            logger.warning(f\"Sensitive information detected in output, filtered: {', '.join(filtered_output)}\")\n",
    "\n",
    "        # Create a new FunctionResult with the filtered output\n",
    "        context.result = FunctionResult(\n",
    "            function=context.function.metadata,\n",
    "            value=filtered_output,\n",
    "            metadata=context.result.metadata if hasattr(context.result, 'metadata') else {}\n",
    "        )\n",
    "\n",
    "async def test_content_filters():\n",
    "\n",
    "    # Register the filters with the kernel\n",
    "    kernel.add_filter(\"function_invocation\", input_filter)\n",
    "    kernel.add_filter(\"function_invocation\", output_filter)\n",
    "\n",
    "    # Create a simple semantic function\n",
    "    echo_prompt = \"{{$input}}\"\n",
    "    echo_fn = kernel.add_function(\n",
    "        prompt=echo_prompt,\n",
    "        function_name=\"echo\",\n",
    "        plugin_name=\"TestPlugin\"\n",
    "    )\n",
    "\n",
    "    # Test with sensitive information\n",
    "    test_inputs = [\n",
    "        \"My credit card number is 4111-1111-1111-1111\",\n",
    "        \"Contact me at john.doe@example.com or call 555-123-4567\",\n",
    "        \"My SSN is 123-45-6789\",\n",
    "        \"Here's my info: john.doe@example.com, 4111-1111-1111-1111, 555-123-4567\",\n",
    "        \"What are the services that you can offer?\"\n",
    "    ]\n",
    "\n",
    "    for input_text in test_inputs:\n",
    "        print(f\"\\nOriginal Input: {input_text}\")\n",
    "        result = await kernel.invoke(echo_fn, input=input_text)\n",
    "        print(f\"Filtered Output: {result}\")\n",
    "\n",
    "await test_content_filters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
